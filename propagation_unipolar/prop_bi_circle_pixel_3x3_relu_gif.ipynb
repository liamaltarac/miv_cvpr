{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116697/675125867.py:5: UserWarning: A NumPy version >=1.23.5 and <2.5.0 is required for this version of SciPy (detected version 1.23.0)\n",
      "  from scipy import ndimage\n",
      "2025-12-03 02:32:59.926929: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-03 02:32:59.952498: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-03 02:32:59.952518: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-03 02:32:59.953355: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-03 02:32:59.957763: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-03 02:33:00.396721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "from skimage.filters import sobel_h\n",
    "from skimage.filters import sobel_v\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.proj3d import proj_transform\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "\n",
    "import scienceplots\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "#plt.rcParams['figure.figsize'] = [10,10]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "from tensorflow.nn import depthwise_conv2d\n",
    "from tensorflow.math import multiply, reduce_sum, reduce_mean,reduce_euclidean_norm, sin, cos, abs\n",
    "from tensorflow import stack, concat, expand_dims\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from utils.utils import *\n",
    "import cv2\n",
    "\n",
    "from scipy import ndimage, fft\n",
    "from io import BytesIO\n",
    "\n",
    "plt.style.use(['science', 'ieee'])\n",
    "plt.rcParams.update({'figure.dpi': '100'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98 100]\n"
     ]
    }
   ],
   "source": [
    "k = 3   # kernel size\n",
    "beta2s = [0, 0.25, 1] #[0, 0.25, 0.75, 1]  \n",
    "activations = [tf.nn.relu]\n",
    "timestamps = np.linspace(0, 100, num=100, dtype=np.int32)\n",
    "print(\"Timestamps: \", timestamps)\n",
    "experiment_name = \"bipolar_circle\"\n",
    "box_dims = [200, 100]\n",
    "step =   25 # Plot axis step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.relu.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3590d4c550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 330x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Single pixel input\n",
    "d = 715\n",
    "mid = d//2\n",
    "\n",
    "img = np.zeros((d,d)) # cv2.imread('input4.png', 0)/255. \n",
    "\n",
    "\n",
    "cv2.circle(img,(mid,mid), 19, (255.0), -1)\n",
    "mid = img.shape[0]//2\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 02:33:01.264803: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-03 02:33:01.305210: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-03 02:33:01.306636: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-03 02:33:01.307329: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2348] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2025-12-03 02:33:01.308290: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-03 02:33:01.309106: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-03 02:33:01.310264: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-03 02:33:01.311285: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2348] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2025-12-03 02:33:01.406969: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-03 02:33:01.407797: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-03 02:33:01.408568: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-03 02:33:01.409383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8843 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5070, pci bus id: 0000:01:00.0, compute capability: 12.0\n"
     ]
    }
   ],
   "source": [
    "filters = np.zeros((3,3,1,1))\n",
    "img = tf.cast(tf.repeat(tf.expand_dims([img], axis=-1) , repeats = filters.shape[-2], axis=-1), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 02:33:01.672964: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2025-12-03 02:33:01.672982: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2025-12-03 02:33:01.672997: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2025-12-03 02:33:01.673017: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2025-12-03 02:33:01.673034: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2025-12-03 02:33:01.673067: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2025-12-03 02:33:01.673814: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2025-12-03 02:33:01.674669: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:225] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 12.0\n",
      "2025-12-03 02:33:01.674678: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:228] Used ptxas at ptxas\n",
      "2025-12-03 02:33:01.674713: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-12-03 02:33:01.674739: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-12-03 02:33:01.674887: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-12-03 02:33:01.674898: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-12-03 02:33:01.674905: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-12-03 02:33:01.674914: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-12-03 02:33:01.674931: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-12-03 02:33:01.679654: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2025-12-03 02:33:01.680758: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 -50\n",
      "39 39\n",
      "tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "0.9999993\n",
      "Step 0 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 02:33:01.910740: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2025-12-03 02:33:01.956970: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2025-12-03 02:33:01.958684: W external/local_xla/xla/stream_executor/gpu/redzone_allocator.cc:322] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2025-12-03 02:33:02.080073: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2025-12-03 02:33:02.081824: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.908737\n",
      "1.0000005\n",
      "Step 1 done.\n",
      "65.53793\n",
      "0.9999999\n",
      "Step 2 done.\n",
      "66.24911\n",
      "0.99999964\n",
      "Step 3 done.\n",
      "66.80411\n",
      "0.99999934\n",
      "Step 4 done.\n",
      "67.28193\n",
      "0.9999993\n",
      "Step 5 done.\n",
      "67.71098\n",
      "1.0000006\n",
      "Step 6 done.\n",
      "68.10561\n",
      "0.9999992\n",
      "Step 7 done.\n",
      "68.47407\n",
      "0.99999964\n",
      "Step 8 done.\n",
      "68.8221\n",
      "1.0000004\n",
      "Step 9 done.\n",
      "69.1535\n",
      "1.0000005\n",
      "Step 10 done.\n",
      "69.47085\n",
      "0.9999999\n",
      "Step 11 done.\n",
      "69.77634\n",
      "1.0000005\n",
      "Step 12 done.\n",
      "70.0717\n",
      "1.0000004\n",
      "Step 13 done.\n",
      "70.35814\n",
      "0.99999946\n",
      "Step 14 done.\n",
      "70.63659\n",
      "0.9999993\n",
      "Step 15 done.\n",
      "70.90812\n",
      "0.9999999\n",
      "Step 16 done.\n",
      "71.17335\n",
      "0.99999976\n",
      "Step 17 done.\n",
      "71.4329\n",
      "0.9999996\n",
      "Step 18 done.\n",
      "71.68727\n",
      "0.9999993\n",
      "Step 19 done.\n",
      "71.93691\n",
      "0.99999917\n",
      "Step 20 done.\n",
      "72.182205\n",
      "1.0000004\n",
      "Step 21 done.\n",
      "72.4236\n",
      "1.0000002\n",
      "Step 22 done.\n",
      "72.661156\n",
      "1.0\n",
      "Step 23 done.\n",
      "72.8952\n",
      "0.99999976\n",
      "Step 24 done.\n",
      "73.125946\n",
      "1.0000001\n",
      "Step 25 done.\n",
      "73.35367\n",
      "0.99999994\n",
      "Step 26 done.\n",
      "73.57828\n",
      "0.99999964\n",
      "Step 27 done.\n",
      "73.79998\n",
      "1.0000001\n",
      "Step 28 done.\n",
      "74.01895\n",
      "0.9999997\n",
      "Step 29 done.\n",
      "74.23502\n",
      "1.0000008\n",
      "Step 30 done.\n",
      "74.44844\n",
      "0.9999999\n",
      "Step 31 done.\n",
      "74.659004\n",
      "0.9999996\n",
      "Step 32 done.\n",
      "74.86679\n",
      "1.0\n",
      "Step 33 done.\n",
      "75.07189\n",
      "0.99999905\n",
      "Step 34 done.\n",
      "75.27405\n",
      "1.0000001\n",
      "Step 35 done.\n",
      "75.47344\n",
      "0.9999991\n",
      "Step 36 done.\n",
      "75.66979\n",
      "1.0000004\n",
      "Step 37 done.\n",
      "75.86326\n",
      "0.99999934\n",
      "Step 38 done.\n",
      "76.05355\n",
      "0.99999964\n",
      "Step 39 done.\n",
      "76.24083\n",
      "1.0000007\n",
      "Step 40 done.\n",
      "76.42491\n",
      "0.99999976\n",
      "Step 41 done.\n",
      "76.60563\n",
      "1.0000001\n",
      "Step 42 done.\n",
      "76.783134\n",
      "0.9999996\n",
      "Step 43 done.\n",
      "76.957115\n",
      "1.0000002\n",
      "Step 44 done.\n",
      "77.12779\n",
      "1.0000006\n",
      "Step 45 done.\n",
      "77.294945\n",
      "0.99999946\n",
      "Step 46 done.\n",
      "77.45842\n",
      "1.0000006\n",
      "Step 47 done.\n",
      "77.6184\n",
      "0.9999997\n",
      "Step 48 done.\n",
      "77.77462\n",
      "1.0000001\n",
      "Step 49 done.\n",
      "77.92727\n",
      "1.0000004\n",
      "Step 50 done.\n",
      "78.07618\n",
      "0.99999934\n",
      "Step 51 done.\n",
      "78.221245\n",
      "1.0000005\n",
      "Step 52 done.\n",
      "78.362625\n",
      "1.0000008\n",
      "Step 53 done.\n",
      "78.50023\n",
      "0.9999998\n",
      "Step 54 done.\n",
      "78.63392\n",
      "1.0000001\n",
      "Step 55 done.\n",
      "78.7639\n",
      "0.99999905\n",
      "Step 56 done.\n",
      "78.88998\n",
      "0.99999934\n",
      "Step 57 done.\n",
      "79.01234\n",
      "1.0000005\n",
      "Step 58 done.\n",
      "79.1309\n",
      "0.9999995\n",
      "Step 59 done.\n",
      "79.24555\n",
      "0.9999999\n",
      "Step 60 done.\n",
      "79.35654\n",
      "1.0000002\n",
      "Step 61 done.\n",
      "79.46376\n",
      "1.0000006\n",
      "Step 62 done.\n",
      "79.56724\n",
      "0.9999996\n",
      "Step 63 done.\n",
      "79.6669\n",
      "0.9999999\n",
      "Step 64 done.\n",
      "79.762985\n",
      "1.0000004\n",
      "Step 65 done.\n",
      "79.85542\n",
      "0.9999999\n",
      "Step 66 done.\n",
      "79.94411\n",
      "1.0000004\n",
      "Step 67 done.\n",
      "80.029335\n",
      "1.0000007\n",
      "Step 68 done.\n",
      "80.11101\n",
      "0.99999976\n",
      "Step 69 done.\n",
      "80.18907\n",
      "1.0\n",
      "Step 70 done.\n",
      "80.26378\n",
      "1.0000004\n",
      "Step 71 done.\n",
      "80.33505\n",
      "0.9999993\n",
      "Step 72 done.\n",
      "80.40286\n",
      "0.9999997\n",
      "Step 73 done.\n",
      "80.46744\n",
      "1.0\n",
      "Step 74 done.\n",
      "80.528755\n",
      "1.0000005\n",
      "Step 75 done.\n",
      "80.58684\n",
      "0.99999934\n",
      "Step 76 done.\n",
      "80.64161\n",
      "1.0000004\n",
      "Step 77 done.\n",
      "80.69337\n",
      "1.0000007\n",
      "Step 78 done.\n",
      "80.74206\n",
      "0.9999998\n",
      "Step 79 done.\n",
      "80.787605\n",
      "1.0000001\n",
      "Step 80 done.\n",
      "80.830284\n",
      "1.0000004\n",
      "Step 81 done.\n",
      "80.870026\n",
      "0.99999946\n",
      "Step 82 done.\n",
      "80.906815\n",
      "0.9999998\n",
      "Step 83 done.\n",
      "80.94087\n",
      "1.0\n",
      "Step 84 done.\n",
      "80.97217\n",
      "1.0000005\n",
      "Step 85 done.\n",
      "81.00076\n",
      "0.9999994\n",
      "Step 86 done.\n",
      "81.02657\n",
      "0.9999997\n",
      "Step 87 done.\n",
      "81.04989\n",
      "1.0000001\n",
      "Step 88 done.\n",
      "81.07065\n",
      "0.99999905\n",
      "Step 89 done.\n",
      "81.088776\n",
      "1.0000001\n",
      "Step 90 done.\n",
      "81.10457\n",
      "1.0000006\n",
      "Step 91 done.\n",
      "81.11796\n",
      "1.000001\n",
      "Step 92 done.\n",
      "81.12899\n",
      "0.9999999\n",
      "Step 93 done.\n",
      "81.1376\n",
      "1.0000001\n",
      "Step 94 done.\n",
      "81.144066\n",
      "1.0000005\n",
      "Step 95 done.\n",
      "81.148346\n",
      "0.99999934\n",
      "Step 96 done.\n",
      "81.15033\n",
      "0.99999976\n",
      "Step 97 done.\n",
      "81.15032\n",
      "1.0000001\n",
      "Step 98 done.\n",
      "81.14826\n",
      "0.99999905\n",
      "Step 100 done.\n",
      "81.144066\n",
      "Saved GIF to figures/bipolar_circle_relu_3x3/bipolar_circle_3x3__relu_0.gif\n",
      "tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "0.9999993\n",
      "Step 0 done.\n",
      "55.346584\n",
      "0.9999996\n",
      "Step 1 done.\n",
      "56.60369\n",
      "0.9999999\n",
      "Step 2 done.\n",
      "57.170902\n",
      "1.0000008\n",
      "Step 3 done.\n",
      "57.60755\n",
      "1.0000005\n",
      "Step 4 done.\n",
      "57.98133\n",
      "1.0\n",
      "Step 5 done.\n",
      "58.31583\n",
      "1.0000004\n",
      "Step 6 done.\n",
      "58.62274\n",
      "0.99999994\n",
      "Step 7 done.\n",
      "58.90883\n",
      "0.99999934\n",
      "Step 8 done.\n",
      "59.178524\n",
      "0.9999996\n",
      "Step 9 done.\n",
      "59.434925\n",
      "0.99999976\n",
      "Step 10 done.\n",
      "59.6802\n",
      "1.0000002\n",
      "Step 11 done.\n",
      "59.91608\n",
      "0.99999964\n",
      "Step 12 done.\n",
      "60.143734\n",
      "1.0\n",
      "Step 13 done.\n",
      "60.364323\n",
      "1.0000002\n",
      "Step 14 done.\n",
      "60.5786\n",
      "1.0000007\n",
      "Step 15 done.\n",
      "60.787273\n",
      "0.99999994\n",
      "Step 16 done.\n",
      "60.990837\n",
      "1.0000002\n",
      "Step 17 done.\n",
      "61.18996\n",
      "1.0000005\n",
      "Step 18 done.\n",
      "61.384884\n",
      "0.9999999\n",
      "Step 19 done.\n",
      "61.576008\n",
      "1.0000004\n",
      "Step 20 done.\n",
      "61.76366\n",
      "0.9999997\n",
      "Step 21 done.\n",
      "61.94809\n",
      "1.0000001\n",
      "Step 22 done.\n",
      "62.12961\n",
      "0.9999996\n",
      "Step 23 done.\n",
      "62.308285\n",
      "1.0000008\n",
      "Step 24 done.\n",
      "62.484417\n",
      "1.0000004\n",
      "Step 25 done.\n",
      "62.658054\n",
      "1.0000005\n",
      "Step 26 done.\n",
      "62.829414\n",
      "1.0000001\n",
      "Step 27 done.\n",
      "62.998547\n",
      "1.0000005\n",
      "Step 28 done.\n",
      "63.165604\n",
      "1.0\n",
      "Step 29 done.\n",
      "63.33063\n",
      "1.0000005\n",
      "Step 30 done.\n",
      "63.4937\n",
      "1.0000007\n",
      "Step 31 done.\n",
      "63.654945\n",
      "0.9999993\n",
      "Step 32 done.\n",
      "63.81414\n",
      "0.99999964\n",
      "Step 33 done.\n",
      "63.971745\n",
      "1.0000001\n",
      "Step 34 done.\n",
      "64.127304\n",
      "1.0000004\n",
      "Step 35 done.\n",
      "64.28137\n",
      "0.99999976\n",
      "Step 36 done.\n",
      "64.43319\n",
      "0.9999993\n",
      "Step 37 done.\n",
      "64.583755\n",
      "0.9999998\n",
      "Step 38 done.\n",
      "64.73194\n",
      "1.0000001\n",
      "Step 39 done.\n",
      "64.87901\n",
      "1.0000005\n",
      "Step 40 done.\n",
      "65.0234\n",
      "1.0000002\n",
      "Step 41 done.\n",
      "65.16696\n",
      "1.0000006\n",
      "Step 42 done.\n",
      "65.307465\n",
      "0.9999998\n",
      "Step 43 done.\n",
      "65.44736\n",
      "0.9999995\n",
      "Step 44 done.\n",
      "65.58385\n",
      "0.9999998\n",
      "Step 45 done.\n",
      "65.72017\n",
      "0.9999993\n",
      "Step 46 done.\n",
      "65.8525\n",
      "0.9999997\n",
      "Step 47 done.\n",
      "65.985146\n",
      "1.0000001\n",
      "Step 48 done.\n",
      "66.11316\n",
      "0.99999964\n",
      "Step 49 done.\n",
      "66.24203\n",
      "1.0\n",
      "Step 50 done.\n",
      "66.36562\n",
      "0.99999964\n",
      "Step 51 done.\n",
      "66.49067\n",
      "0.9999999\n",
      "Step 52 done.\n",
      "66.60967\n",
      "0.99999964\n",
      "Step 53 done.\n",
      "66.730865\n",
      "0.9999999\n",
      "Step 54 done.\n",
      "66.845184\n",
      "0.99999946\n",
      "Step 55 done.\n",
      "66.962456\n",
      "0.99999976\n",
      "Step 56 done.\n",
      "67.07199\n",
      "0.9999993\n",
      "Step 57 done.\n",
      "67.18532\n",
      "0.9999997\n",
      "Step 58 done.\n",
      "67.289986\n",
      "0.9999993\n",
      "Step 59 done.\n",
      "67.39937\n",
      "0.99999964\n",
      "Step 60 done.\n",
      "67.49908\n",
      "0.9999991\n",
      "Step 61 done.\n",
      "67.60449\n",
      "1.0000005\n",
      "Step 62 done.\n",
      "67.69929\n",
      "1.0000001\n",
      "Step 63 done.\n",
      "67.80074\n",
      "1.0000005\n",
      "Step 64 done.\n",
      "67.89046\n",
      "1.0000001\n",
      "Step 65 done.\n",
      "67.987976\n",
      "1.0000004\n",
      "Step 66 done.\n",
      "68.07263\n",
      "0.99999994\n",
      "Step 67 done.\n",
      "68.16625\n",
      "1.0000004\n",
      "Step 68 done.\n",
      "68.245865\n",
      "0.9999998\n",
      "Step 69 done.\n",
      "68.335625\n",
      "0.99999934\n",
      "Step 70 done.\n",
      "68.4102\n",
      "0.9999998\n",
      "Step 71 done.\n",
      "68.49614\n",
      "1.0000004\n",
      "Step 72 done.\n",
      "68.56579\n",
      "1.0000006\n",
      "Step 73 done.\n",
      "68.64796\n",
      "1.0000002\n",
      "Step 74 done.\n",
      "68.71257\n",
      "0.9999997\n",
      "Step 75 done.\n",
      "68.791046\n",
      "1.0000001\n",
      "Step 76 done.\n",
      "68.850746\n",
      "0.9999998\n",
      "Step 77 done.\n",
      "68.9256\n",
      "1.0\n",
      "Step 78 done.\n",
      "68.98044\n",
      "0.9999995\n",
      "Step 79 done.\n",
      "69.05172\n",
      "1.0000001\n",
      "Step 80 done.\n",
      "69.10185\n",
      "1.0000005\n",
      "Step 81 done.\n",
      "69.16966\n",
      "0.99999994\n",
      "Step 82 done.\n",
      "69.21502\n",
      "1.0000004\n",
      "Step 83 done.\n",
      "69.27943\n",
      "0.9999999\n",
      "Step 84 done.\n",
      "69.320175\n",
      "0.9999995\n",
      "Step 85 done.\n",
      "69.38126\n",
      "0.9999998\n",
      "Step 86 done.\n",
      "69.4175\n",
      "1.0000002\n",
      "Step 87 done.\n",
      "69.47541\n",
      "0.99999976\n",
      "Step 88 done.\n",
      "69.50722\n",
      "1.0000002\n",
      "Step 89 done.\n",
      "69.56192\n",
      "0.99999976\n",
      "Step 90 done.\n",
      "69.589424\n",
      "1.0000002\n",
      "Step 91 done.\n",
      "69.64107\n",
      "0.9999995\n",
      "Step 92 done.\n",
      "69.66433\n",
      "0.9999992\n",
      "Step 93 done.\n",
      "69.71301\n",
      "1.0000005\n",
      "Step 94 done.\n",
      "69.73225\n",
      "1.0000001\n",
      "Step 95 done.\n",
      "69.77804\n",
      "0.99999976\n",
      "Step 96 done.\n",
      "69.79321\n",
      "1.0\n",
      "Step 97 done.\n",
      "69.83621\n",
      "0.99999964\n",
      "Step 98 done.\n",
      "69.847496\n",
      "1.0\n",
      "Step 100 done.\n",
      "69.88779\n",
      "Saved GIF to figures/bipolar_circle_relu_3x3/bipolar_circle_3x3__relu_0.25.gif\n",
      "tf.Tensor(0.9999999999999999, shape=(), dtype=float64)\n",
      "0.9999993\n",
      "Step 0 done.\n",
      "26.090633\n",
      "0.9999988\n",
      "Step 1 done.\n",
      "120.93401\n",
      "1.0\n",
      "Step 2 done.\n",
      "147.87537\n",
      "1.0\n",
      "Step 3 done.\n",
      "163.47601\n",
      "1.0\n",
      "Step 4 done.\n",
      "173.43854\n",
      "1.0000005\n",
      "Step 5 done.\n",
      "180.81863\n",
      "1.0000001\n",
      "Step 6 done.\n",
      "186.25009\n",
      "1.0000004\n",
      "Step 7 done.\n",
      "190.45403\n",
      "1.0000005\n",
      "Step 8 done.\n",
      "193.74097\n",
      "0.99999994\n",
      "Step 9 done.\n",
      "196.21417\n",
      "1.0\n",
      "Step 10 done.\n",
      "198.02115\n",
      "1.0000008\n",
      "Step 11 done.\n",
      "199.23805\n",
      "1.0\n",
      "Step 12 done.\n",
      "200.00754\n",
      "0.9999998\n",
      "Step 13 done.\n",
      "200.38054\n",
      "0.9999999\n",
      "Step 14 done.\n",
      "200.43701\n",
      "0.99999976\n",
      "Step 15 done.\n",
      "200.1969\n",
      "1.0\n",
      "Step 16 done.\n",
      "199.71106\n",
      "0.99999994\n",
      "Step 17 done.\n",
      "199.02858\n",
      "1.0\n",
      "Step 18 done.\n",
      "198.18846\n",
      "1.0\n",
      "Step 19 done.\n",
      "197.22308\n",
      "1.000001\n",
      "Step 20 done.\n",
      "197.57288\n",
      "0.9999998\n",
      "Step 21 done.\n",
      "198.70238\n",
      "0.9999998\n",
      "Step 22 done.\n",
      "199.6892\n",
      "0.9999999\n",
      "Step 23 done.\n",
      "200.54236\n",
      "0.99999994\n",
      "Step 24 done.\n",
      "201.26677\n",
      "1.0000007\n",
      "Step 25 done.\n",
      "201.88013\n",
      "0.9999995\n",
      "Step 26 done.\n",
      "202.39525\n",
      "0.9999995\n",
      "Step 27 done.\n",
      "202.82388\n",
      "0.9999996\n",
      "Step 28 done.\n",
      "203.1741\n",
      "1.0000004\n",
      "Step 29 done.\n",
      "203.45502\n",
      "1.0000004\n",
      "Step 30 done.\n",
      "203.67249\n",
      "1.0000004\n",
      "Step 31 done.\n",
      "203.83006\n",
      "0.99999934\n",
      "Step 32 done.\n",
      "203.9319\n",
      "1.0000002\n",
      "Step 33 done.\n",
      "203.9852\n",
      "1.0000001\n",
      "Step 34 done.\n",
      "203.9953\n",
      "1.0000001\n",
      "Step 35 done.\n",
      "203.96722\n",
      "1.0000001\n",
      "Step 36 done.\n",
      "203.90503\n",
      "1.0\n",
      "Step 37 done.\n",
      "203.81203\n",
      "0.9999999\n",
      "Step 38 done.\n",
      "203.68977\n",
      "0.99999994\n",
      "Step 39 done.\n",
      "203.54019\n",
      "0.99999976\n",
      "Step 40 done.\n",
      "203.36629\n",
      "0.9999996\n",
      "Step 41 done.\n",
      "203.17068\n",
      "0.9999997\n",
      "Step 42 done.\n",
      "202.95628\n",
      "0.9999997\n",
      "Step 43 done.\n",
      "202.72516\n",
      "1.0000005\n",
      "Step 44 done.\n",
      "202.47893\n",
      "1.0000004\n",
      "Step 45 done.\n",
      "202.21834\n",
      "0.9999994\n",
      "Step 46 done.\n",
      "201.9438\n",
      "0.99999934\n",
      "Step 47 done.\n",
      "201.65765\n",
      "1.0000002\n",
      "Step 48 done.\n",
      "201.36113\n",
      "1.0000002\n",
      "Step 49 done.\n",
      "201.05554\n",
      "1.0000002\n",
      "Step 50 done.\n",
      "200.74207\n",
      "0.9999991\n",
      "Step 51 done.\n",
      "200.42154\n",
      "1.0\n",
      "Step 52 done.\n",
      "200.0947\n",
      "0.99999994\n",
      "Step 53 done.\n",
      "199.7616\n",
      "0.99999994\n",
      "Step 54 done.\n",
      "199.42308\n",
      "1.0000008\n",
      "Step 55 done.\n",
      "199.08\n",
      "0.99999964\n",
      "Step 56 done.\n",
      "198.73283\n",
      "0.9999997\n",
      "Step 57 done.\n",
      "198.38269\n",
      "0.99999964\n",
      "Step 58 done.\n",
      "198.02982\n",
      "1.0000005\n",
      "Step 59 done.\n",
      "197.67441\n",
      "1.0000005\n",
      "Step 60 done.\n",
      "197.31654\n",
      "0.99999946\n",
      "Step 61 done.\n",
      "196.95656\n",
      "0.9999995\n",
      "Step 62 done.\n",
      "196.59537\n",
      "1.0000002\n",
      "Step 63 done.\n",
      "196.23312\n",
      "1.0000002\n",
      "Step 64 done.\n",
      "195.8702\n",
      "1.0000002\n",
      "Step 65 done.\n",
      "195.50694\n",
      "0.99999917\n",
      "Step 66 done.\n",
      "195.14319\n",
      "0.9999999\n",
      "Step 67 done.\n",
      "194.77933\n",
      "1.0000001\n",
      "Step 68 done.\n",
      "194.41553\n",
      "1.0\n",
      "Step 69 done.\n",
      "194.05194\n",
      "1.0000008\n",
      "Step 70 done.\n",
      "193.68884\n",
      "0.99999976\n",
      "Step 71 done.\n",
      "193.32625\n",
      "0.9999998\n",
      "Step 72 done.\n",
      "192.96469\n",
      "0.9999998\n",
      "Step 73 done.\n",
      "192.60397\n",
      "1.0000006\n",
      "Step 74 done.\n",
      "192.24413\n",
      "1.0000005\n",
      "Step 75 done.\n",
      "191.88527\n",
      "0.99999946\n",
      "Step 76 done.\n",
      "191.52742\n",
      "0.99999946\n",
      "Step 77 done.\n",
      "191.1711\n",
      "1.0000002\n",
      "Step 78 done.\n",
      "190.8162\n",
      "1.0000004\n",
      "Step 79 done.\n",
      "190.4628\n",
      "1.0000004\n",
      "Step 80 done.\n",
      "190.1109\n",
      "0.9999992\n",
      "Step 81 done.\n",
      "189.76025\n",
      "1.0\n",
      "Step 82 done.\n",
      "189.4114\n",
      "1.0000001\n",
      "Step 83 done.\n",
      "189.06422\n",
      "1.0\n",
      "Step 84 done.\n",
      "188.71873\n",
      "1.0\n",
      "Step 85 done.\n",
      "188.37506\n",
      "1.0000007\n",
      "Step 86 done.\n",
      "188.0332\n",
      "0.9999998\n",
      "Step 87 done.\n",
      "187.69292\n",
      "0.99999976\n",
      "Step 88 done.\n",
      "187.35461\n",
      "0.9999997\n",
      "Step 89 done.\n",
      "187.01816\n",
      "1.0000005\n",
      "Step 90 done.\n",
      "186.6836\n",
      "1.0000006\n",
      "Step 91 done.\n",
      "186.35095\n",
      "0.99999946\n",
      "Step 92 done.\n",
      "186.02003\n",
      "0.9999994\n",
      "Step 93 done.\n",
      "185.69125\n",
      "1.0000004\n",
      "Step 94 done.\n",
      "185.36443\n",
      "1.0000004\n",
      "Step 95 done.\n",
      "185.0395\n",
      "1.0000002\n",
      "Step 96 done.\n",
      "184.7165\n",
      "1.0000002\n",
      "Step 97 done.\n",
      "184.39548\n",
      "0.99999917\n",
      "Step 98 done.\n",
      "184.0762\n",
      "1.0000001\n",
      "Step 100 done.\n",
      "183.75912\n",
      "Saved GIF to figures/bipolar_circle_relu_3x3/bipolar_circle_3x3__relu_1.gif\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import imageio.v2 as imageio\n",
    "from io import BytesIO\n",
    "\n",
    "filters = np.zeros((3,3,1,1))\n",
    "\n",
    "# Get image height and width\n",
    "vals_x = img[0, img.shape[1]//2, :, :]\n",
    "vals_y = img[0, :, img.shape[2]//2, :]\n",
    "nonzeros = np.nonzero(vals_x)[0]\n",
    "if nonzeros.size > 0:\n",
    "    length_y = nonzeros[-1] - nonzeros[0] + 1\n",
    "else:\n",
    "    length_y = 0  # all zeros\n",
    "nonzeros = np.nonzero(vals_x)[0]\n",
    "if nonzeros.size > 0:\n",
    "    length_x = nonzeros[-1] - nonzeros[0] + 1\n",
    "else:\n",
    "    length_x = 0  # all zeros\n",
    "\n",
    "# step should be defined somewhere above in your code\n",
    "# step = 2**np.floor(np.log2(((length_x) + np.max(timestamps)) // 5))\n",
    "\n",
    "max_x = int(box_dims[0] * 0.75)\n",
    "min_x = -int(box_dims[0] * 0.25)\n",
    "\n",
    "max_y = box_dims[1] // 2\n",
    "min_y = -(box_dims[1] // 2)\n",
    "\n",
    "print(max_y, min_y)\n",
    "print(length_x, length_y)\n",
    "\n",
    "for activation in activations:\n",
    "    directory = f\"figures/{experiment_name}_{activation.__name__}_{k}x{k}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for beta2 in beta2s:\n",
    "\n",
    "        # ---- frames for this (activation, beta2) pair ----\n",
    "        frames = []\n",
    "\n",
    "        filters = np.zeros((3,3,1,1))\n",
    "\n",
    "        t = np.zeros((3,3))\n",
    "        t[1, 0] = np.sqrt(beta2)\n",
    "        t[0, 0] = np.sqrt(1 - beta2)\n",
    "        filters = np.reshape(fft.idctn(t, norm='ortho'), (3,3,1,1))\n",
    "        print(tf.math.reduce_euclidean_norm(filters))\n",
    "        # filters /= tf.math.reduce_euclidean_norm(filters)\n",
    "\n",
    "        w = tf.cast(filters, dtype=tf.float32)\n",
    "        w = tf.transpose(w, perm=(1, 0, 2, 3))\n",
    "\n",
    "        x = img\n",
    "\n",
    "        # evolve in time\n",
    "        png_counter = 0\n",
    "        for i in timestamps:\n",
    "            x = x / np.std(x)\n",
    "\n",
    "            # --- single-frame figure for this timestamp ---\n",
    "            fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "            print(np.std(x))\n",
    "            vals = x[0, x.shape[1]//2, :, :]\n",
    "            vals = vals / np.sum(vals)\n",
    "\n",
    "            pos = np.expand_dims(\n",
    "                np.linspace(-(x.shape[1]//2), x.shape[1]//2, x.shape[1]),\n",
    "                -1\n",
    "            )\n",
    "            mean = tf.reduce_sum(pos * vals)\n",
    "            var = tf.reduce_sum(((pos - mean)**2) * vals)\n",
    "            std = np.sqrt(var)\n",
    "\n",
    "            im = ax.imshow(x[0, :, :, 0])\n",
    "            ax.plot(mid, mid, marker='+', color='red')\n",
    "\n",
    "            ax.annotate(r'$\\sigma_x=$' + f\"{std:.2f}\",\n",
    "                        (0.05, 0.01),\n",
    "                        ha='left', va='bottom',\n",
    "                        color='white',\n",
    "                        xycoords='axes fraction',\n",
    "                        fontsize=15)\n",
    "            ax.annotate(r'$\\mu_x=$' + f\"{mean:.2f}\",\n",
    "                        (0.05, 0.14),\n",
    "                        ha='left', va='bottom',\n",
    "                        color='white',\n",
    "                        xycoords='axes fraction',\n",
    "                        fontsize=15)\n",
    "\n",
    "            ax.set(\n",
    "                xlim=((mid + min_x, mid + max_x)),\n",
    "                ylim=((mid + min_y, mid + max_y))\n",
    "            )\n",
    "\n",
    "            # re-label ticks relative to midpoint\n",
    "            xticks = np.arange(\n",
    "                np.sign(min_x) * (np.abs(min_x)//2) * 2,\n",
    "                step + (max_x//2) * 2,\n",
    "                step=step\n",
    "            ) + mid\n",
    "            yticks = np.arange(\n",
    "                np.sign(min_y) * (np.abs(min_y)//2) * 2,\n",
    "                step + (max_y//2) * 2,\n",
    "                step=step\n",
    "            ) + mid\n",
    "\n",
    "            ax.set_xticks(xticks)\n",
    "            ax.set_yticks(yticks)\n",
    "            ax.tick_params(axis='x', labelsize=15)\n",
    "            ax.tick_params(axis='y', labelsize=15)\n",
    "\n",
    "            ax.set_xticklabels([x_tick - mid for x_tick in xticks])\n",
    "            ax.set_yticklabels([y_tick - mid for y_tick in yticks])\n",
    "\n",
    "            if beta2 == 0:\n",
    "                ax.set_title(f\"t={i}\", fontsize=25)\n",
    "\n",
    "            fig.tight_layout()\n",
    "\n",
    "            # ---- capture frame into memory ----\n",
    "            buf = BytesIO()\n",
    "            fig.savefig(buf, format=\"png\", dpi=fig.dpi, bbox_inches=\"tight\")\n",
    "            buf.seek(0)\n",
    "            frame = imageio.imread(buf)\n",
    "            frames.append(frame)\n",
    "\n",
    "            # ALSO SAVE INDIVIDUAL FRAMES AS PNG\n",
    "            frame_dir = os.path.join(directory, f\"frames_{beta2}\")\n",
    "            os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "            # Save first frame 50 times\n",
    "            if i == timestamps[0]:\n",
    "                repeats = 50\n",
    "            else:\n",
    "                repeats = 1\n",
    "\n",
    "            '''for _ in range(repeats):\n",
    "                png_path = os.path.join(\n",
    "                    frame_dir,\n",
    "                    f\"frame_{activation.__name__}_{beta2}_{png_counter}.png\"\n",
    "                )\n",
    "\n",
    "                fig.savefig(png_path, format=\"png\", dpi=fig.dpi, bbox_inches=\"tight\")\n",
    "                png_counter += 1'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            plt.close(fig)\n",
    "            print(f\"Step {i} done.\")\n",
    "            x = activation(\n",
    "                tf.nn.conv2d(x, w, strides=(1, 1), padding='SAME')\n",
    "            )\n",
    "            w = np.rot90(w, k=2, axes=(0,1))\n",
    "\n",
    "            print(np.max(x))\n",
    "\n",
    "        # ---- save GIF for this (activation, beta2) ----\n",
    "        # --- stay longer on the first frame (pad with copies) ---\n",
    "        hold_first = 50  # increase for longer pause\n",
    "        from PIL import Image\n",
    "\n",
    "        # ---- save GIF for this (activation, beta2) ----\n",
    "        if len(frames) > 0:\n",
    "            # Ensure uint8 for Pillow\n",
    "            frames_uint8 = []\n",
    "            for f in frames:\n",
    "                if f.dtype != np.uint8:\n",
    "                    # assume f is float in [0,1] or [0,255]; adjust as needed\n",
    "                    f_norm = f\n",
    "                    if f_norm.max() <= 1.5:\n",
    "                        f_norm = (f_norm * 255.0)\n",
    "                    frames_uint8.append(f_norm.astype(np.uint8))\n",
    "                else:\n",
    "                    frames_uint8.append(f)\n",
    "\n",
    "            pil_frames = [Image.fromarray(f) for f in frames_uint8]\n",
    "\n",
    "            # durations in *milliseconds* per frame\n",
    "            first_duration_ms = 1000   # 5 seconds for first frame\n",
    "            rest_duration_ms  = 100    # 0.3 seconds for others\n",
    "\n",
    "            durations_ms = [first_duration_ms] + [rest_duration_ms] * (len(pil_frames) - 1)\n",
    "\n",
    "            gif_path = os.path.join(\n",
    "                directory,\n",
    "                f\"{experiment_name}_{k}x{k}__{activation.__name__}_{beta2}.gif\"\n",
    "            )\n",
    "\n",
    "            pil_frames[0].save(\n",
    "                gif_path,\n",
    "                save_all=True,\n",
    "                append_images=pil_frames[1:],\n",
    "                duration=durations_ms,   # per-frame durations\n",
    "                loop=0                   # 0 = infinite loop\n",
    "            )\n",
    "            print(f\"Saved GIF to {gif_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>,\n",
       " <PIL.Image.Image image mode=RGBA size=586x304>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pil_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([307, 332, 357, 382, 407])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yticks#-mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
